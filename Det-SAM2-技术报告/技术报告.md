### Det-SAM2-技术报告



![sam2模型结构图2](./asset/sam2模型结构图2.png)

图注：SAM2论文中展示的模型框架图，[Meta Segment Anything Model 2](https://ai.meta.com/sam2/)



- SAM2原始框架

![SAM2框架图](./asset/SAM2框架图.jpg)

图注：SAM2原始框架图，SAM2原文中提到的但是未在SAM2原文图中绘制的是：Memory Decoder不仅接收来自Mask Decoder的输出结果，也接收来自Image Encoder的输出结果。路径如图中下方的跳跃连接。



#### 1.Det-SAM2试验Demo

我们希望通过检测模型自动地为SAM2提供条件提示。
Det-SAM2试验Demo：仅在其中一帧上运行检测模型（第一帧提供条件为条件帧，后续帧都是非条件帧）：

![Det-SAM2只在第一帧给提示框架图](./asset/Det-SAM2只在第一帧给提示框架图.jpg)

图注：Det-SAM2试验Demo框架图。自动地为某一帧添加条件提示，条件提示由检测模型（这里使用的是YOLOv8）给出，检测框结果信息作为框提示输入给Prompt Encoder。



#### 2. 接收在线视频流的实时处理框架Det-SAM2

官方demo只支持读取视频路径，处理预先完结的离线视频。我们希望实现加载视频流，不断对新帧进行推理，且每一帧都运行我们检测模型，为SAM2添加条件提示。

首先我们在流程上的Det-SAM2框架如下：
![Det-SAM2框架图](./asset/Det-SAM2框架图.jpg)

图注：Det-SAM2框架允许自动地为每一帧都添加条件提示

在接收视频流的时间维度上Det-SAM2处理方式如下：
![视频流处理流程图](./asset/视频流处理流程图.jpg)

图注：处理视频流示意图。每一帧都经过Detection Model作为SAM2的条件帧（图中青绿色），随后在整个已有视频帧中进行传播推理propagate in video（图中黄色）。

在实际情况中，传播过程（propagate in video）占用了相当长的时间。在传播过程中，每一次新增视频推理，都会对此前所有帧执行一次SAM2推理。这使得我们对长度为N帧的视频应用当前推理框架时，会对$$ \frac{1}{2} N^2 $$的帧执行推理。



#### 3. 累积视频流和间隔检测模型的条件提示输入

我们希望在接收视频流的基础上，允许新进来的视频帧累积再视频帧缓存中。我们每次送给Det-SAM2框架进行推理的内容不要一帧一帧地频繁给，而是一次送进去一个视频帧累积的序列，这样SAM2一次处理多个帧，减少传播次数。
同时我们还希望间隔检测模型的推理，不需要送给SAM2的每一帧都添加检测模型的条件。经过上一步Demo的实验，我们发现如果每一帧都施加条件的话，SAM2无法修正来自条件帧的内容。

视频流累积和间隔检测条件提示的流程示意图：

![视频流累积处理+间隔检测流程图](./asset/视频流累积处理+间隔检测流程图.jpg)

图注：Det-SAM2中视频流累积和间隔检测条件提示的流程图。对于视频流接收到的每一帧，会先累积到帧缓存中，当人为设置存储大小的帧缓存累积了一定帧时，则送给Det-SAM2框架进行推理。在推理当前累积的视频序列时，会根据间隔设置判断那些帧是条件帧(cond-frame)那些帧是非条件帧(non-cond-frame)。条件帧则由检测模型提供提示，由SAM2的Prompt Encoder提供特征嵌入。

我们通过累积一定数量的视频帧再进行推理可以极大地缓解SAM2传播的次数。假设我们缓存K帧一并进行推理，则整个长度为N帧的视频执行的传播propagate in video过程只需要对大约$$ \frac{1}{2K} N^2 $$帧进行处理。



#### 4. 限制视频传播propagate in video的长度

我们在了解propagate in video传播机制每次推理都会对所有历史帧中的非条件帧non-cond-frames进行追踪，这也是SAM2之所以能够修正的机制，当获取到新的条件帧时，传播机制可以依赖条件帧的信息对过去的所有非条件帧二次推理并将结果覆盖。
然而，在Det-SAM2流程的实际情况中，需要修正的非条件帧与其所依赖的条件帧之间不会间隔太远。因此我们可以限制每次propagate in video传播时推理所涉及到的帧数。在视频流过程中，相对于本次推理，遥远的视频开始的大多数帧都是已经确定且不需要被修正的，越是靠近新时刻的推理结果越有可能在未来的推理中被颠覆。
因此我们需要对propagate in video传播操作做如下限制：

- 传播中追踪处理顺序为倒序处理，起始帧为当前最新帧
- 传播中限制追踪处理的最大视频帧数，但是本次传播处理的长度（极端情况下）应当至少包含上一次推理的累积视频帧序列，否则传播将不具备任何修正意义。

限制传播长度的流程示意图如下：

![限制最大传播长度](./asset/限制最大传播长度.jpg)

图注：限制传播长度的流程示意图。propagate in video部分执行时只追踪人为设置的固定长度，而不需要每次传播都重复追踪视频靠前的帧。

增加视频传播的最大长度可以扩大修正范围，但也会带来更大的计算开销。减小视频传播的最大长度则可以带来更快的推理速度，但也会降低视频流中条件帧能够修正非条件帧的最大范围。

通过限制传播长度为M，累积帧缓存为K，我们在推理长度为N帧的视频时只需要对大约$$ \frac{M}{K} N $$帧做处理。



#### 5. 预加载Memory Bank内存库

SAM2能将图像分割能力迁移到视频分割领域的一个重要改进就是增加了Memory Bank可以提供帧与帧之间的关联（Memory Attention）。然而，Memory Bank内存库的生成与构造是在线的，基于当前传入的视频中已加载的帧（每次加载帧时会将条件提示、掩码输出与原始输入添加在内存库中）。

我们希望能够通过预加载一个在旧视频中已经构建好的内存库，从而可以在新的视频上无需条件提示直接进行推理。即允许SAM2预先加载一个离线内存库，该离线内存库经过精心设计，包含了新视频中可能会需要的所有提示和难样本提示示例，就像“系统提示词”一样。在之后新视频的推理中，新生成的Memory会累加在预加载的内存库上。新生成的Memory与预加载的Memory在人为概念上区分，但是在SAM2实际推理中没有区别。

预加载内存库流程示意图如下：![预加载内存库](./asset/预加载内存库.jpg)

图注：预加载内存库流程图。Memory Bank预先加载一个离线内存库，源自此前推理过的视频。至此，本次推理的时候可以直接将先前视频中的推理记忆作用于新视频的推理。

在具体实现上，SAM2代码中的inference_state存有memory bank中所有的信息，因此将inference_state迁移到新视频推理中，且新视频不再重新初始化init_state即可。



