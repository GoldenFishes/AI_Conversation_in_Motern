#### 1. 算法进展回顾

![image-20240711104130491](.\asset\image-20240711104130491.png)

任何时候算法进展，请关注：
1.算法进展表：https://alidocs.dingtalk.com/i/nodes/N7dx2rn0JbNZP2xRtdlqAyo7JMGjLRb3?doc_type=wiki_notable&iframeQuery=sheetId%3Da5mpmcw3o0a3agno7gde9%26viewId%3DRe1tn0o&rnd=0.8072404881274278

2.技术报告：[代码浏览 - hi_light - motern_ai - MOTERN (coding.net)](https://serverless-100013832940.coding.net/p/motern_ai/d/hi_light/git/tree/master/HiLight_Tech_Report)



##### 1.1 HiLight 视频问答模型 近期事项

![image-20240711114710472](.\asset\image-20240711114710472.png)

1.【模型性能检查（已完成）】：

- 排查问题，发现并解决了一个漏洞（CLIP-ViP权重在每次加载都重新初始化的问题）

- 实验比较CLIP-ViP Vision Encoder的三种不同的权重在下游任务上的表现情况

  三组对照实验均在VideoInstruct100K训练，分别在VideoInstruct100K的子集和私有数据集vqa_table上验证

  ![image-20240711111745439](.\asset\image-20240711111745439.png)

  在VideoInstruct100K训练；VideoInstruct100k上验证：
  CLIP-ViP-原始权重 优于 CLIP-ViP-table10k 优于 CLIP-ViP-table1k。

  ![image-20240711111802616](.\asset\image-20240711111802616.png)在VideoInstruct100K训练；vqa_table上验证：

  CLIP-ViP-table1k权重 优于 CLIP-ViP-table10k权重 优于 CLIP-ViP-原始权重

  在不同验证集中， CLIP-ViP-table10k始终保持在相对中间的水准。CLIP-ViP-原始权重的泛化性较好，CLIP-ViP-table1k在台球场景表现较好。因此我们决定带着CLIP-ViP-原始权重和CLIP-ViP-table1k权重进行后续操作。

  

- 重新确定TokenMining的设计（未消融实验验证）

  1. 有实验表明ImageLM中一张图像对应N个VisionToken时，N从1->64增长时下游任务效果随之提升，当VisionToken数大于64时，继续增大用来表示同一张图像的VisionToken数不会得到明显的收益。大多ImageLM每张图像采用16个以下的VisionToken来表示。
     （[What matters when building vision-language models?]https://arxiv.org/abs/2405.02246）

     

     在VideoLM中每一帧都算作一张图像，如果用来表示每个采样帧的VisionToken数过高，那么光是VisionToken的总数就占据LLM上下文长度相当大的空间。因此许多VideoLM例如Video-LLAMA，Video-ChatGPT都采用一帧对应4个VisionToken的设置，1frame-4token。同时例如VideoGPT+等模型还会通过Token Avg Pooling的操作进一步降低最终LLM需要接收的VisionToken数。

  

  ![image-20240711115348937](.\asset\image-20240711115348937.png)

  

  2. 大部分开源VideoLM的连接件部分都采用mlp2gelu，两层MLP的结构，以我们的表示为 2Linear

  ```python
  # TokenMining结构与参数量情况
  	# CA + 1Linear(512-2048): 2,105,856
      # CA + 2Linear(512-2048-2048): 6,302,208
      # Projector(1frame-2token) + CA + 2Linear(512-2048-2048): 6,892,804
      # CA + 2Linear(512-512-2048): 2,368,512
      # Projector(1frame-2token) + CA + 2Linear(512-512-2048): 2,959,108
      # Projector(1frame-8token) + CA + 2Linear(512-512-2048): 2,959,120
      # Projector(1frame-8token) + CA + 2Linear(512-1024-2048): 4,270,352
  ```

  

2.【视频对话数据集制作2（进行中）】：

- 1.提高指令微调数据集的质量的数量，vqa_table数据集数量持续增加中。

  专有场景微调数据集，使用自己的vqa_table数据集。修正数据集错误样本，优化数据集质量。修正前后微调loss曲线：
  ![vqa_table优化前后](.\asset\vqa_table优化前后.png)

  

- 2.提高预训练数据集规模（Videogpt的数据集规模）

  已有VideoInstruck100K数据集（160G），VideoChatGPT的预训练数据集。现追加获取Valley700K数据集（460G），源自字节近期工作。

至此第一阶段TokenMining训练将使用Valley700K，第二阶段指令微调将采用试验两种方案同时进行：

1，VideoInstruck100K对LLM LoRA指令微调 2epoch + vqa_table1k专有场景微调 1epoch ；

2，VideoInstruck100K与vqa_table1k混合数据集进行微调 2epoch。

算上两个Encoder权重，是四组训练。

#### 2. 前沿讨论

##### 2.1 对于视频模态采样的改进——动态采样

视频采样主流方法都是均匀采样，这种采样方式在长视频中会造成非关键帧采样浪费与关键帧采样信息缺乏。因此动态采样是一个很好的改进措施。



【相关实现】：MGSampler: An Explainable Sampling Strategy for Video Action Recognition https://arxiv.org/pdf/2104.09952.pdf

中南大学的一篇早期工作，21年他们从人体行为识别的角度来考虑的，使用图片差或特征差刻画运动信息（量化运动信息），根据运动信息量的概率分布，采用均分累积运动信息的策略，每个分组的运动信息是相同的，然后在从每个分组随机一帧，这样能自适应地采样出区别帧。

【相关实现2】：VideoTree：https://arxiv.org/abs/2405.19209 采用聚类和树优化来决定采样帧的数量和位置

VLM长视频问答会进行密集帧采样，然而许多视频帧往往是冗余的，并且包含不相关的信息，使得密集采样效率低下，并且忽略了视频问答需要不同粒度的事实，一些视频片段与问题高度相关（密集采样），而另一些则不那么相关（稀疏采样）。
 VideoTree引入了动态地从输入视频中提取与问题相关的信息和一个基于树结构的长视频理解框架。

具体过程：首先，VideoTree 根据视觉特征自适应地选择字幕帧，根据视觉特征进行聚类，根据视觉特征与问题的相关性进行聚类。迭代这个过程，直到提取出足够的与查询相关的关键帧。其次，它将可视化聚类组织成一个自适应的层次化树结构; 该树结构表示了不同粒度级别的帧，在相关片段上具有更高(更深)的分辨率。最后，VideoTree 通过遍历树的关键帧并将它们的标题传递给一个 LLM 应答模型来生成每个问题的答案，该模型回答问题。





##### 2.2 AI教练2期需求——捕捉球杆姿态（刚性物体姿态估计）

2D-3D位姿估计任务能很好地概述计量球杆朝向的需求。

![img](.\asset\3eb7ce810c8843c8a7cebf115eb4ef7a.png)

【相关实现】：[[2312.08344\] FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects (arxiv.org)](https://arxiv.org/abs/2312.08344)

1. 基于大语言模型LLM和扩散模型，通过文字信息，引导生成指定的物体3D模型纹理和外观，生成大规模训练数据。
2. 在没有物体的3D CAD模型的情况下，通过神经隐式场表示来进行高效的物体建模。
3. 生成多个假设的姿态，主要有两步过程，生成初始姿态和姿态细化
4. 最后选出最准确的姿态。



【相关实现】： [[2310.00463\] Diff-DOPE: Differentiable Deep Object Pose Estimation (arxiv.org)](https://arxiv.org/abs/2310.00463)

视频，数据和代码：[Diff-DOPE: Differentiable Deep Object Pose Estimation (diffdope.github.io)](https://diffdope.github.io/)

6-DoF 姿态校正器。许多方法将位姿估计问题分为两个阶段：首先是大致估算姿势，然后是细化姿势。本文的重点是后一个姿态细化步骤，这往往是获得良好效果的关键。利用可微分渲染技术的最新进展，将 6-DoF 物体姿态细化问题作为直接的端到端优化进行探索。这种方法的灵感来源于这样一个事实：即使姿势估计稍有错误，产生的重投影错位也很容易察觉，这表明即使没有学习到的先验信息，局部像素信息也足以进行高质量的姿势估计。

![img](.\asset\14fa6082283b4561b014ca21900d5281.png)



##### 2.3 三维空间语义感知

三维场景下语义感知，结合CLIP，SAM 和一个三维重建方法例如3D GS，可以实现三维场景重建基础上的开放词汇语义分割

![teaser.png](.\asset\teaser.png)



【相关实现】：LangSplat 在三维空间内进行精确的开放词汇查询：https://arxiv.prg/pdf/2312.1608v2

在潜在空间中结合SAM和CLIP与3D GS。实现重建三维空间语义场：

1 将32*32点提示的常规网格输入SAM，以获得三个不同语义层次下的掩码，分别代表子部分、部分和整体层次的编码。然后基于SAM预测的IoU分值、稳定性分值和掩码之间的重叠率，为每一组掩码去除冗余的掩码。
每个过滤后的掩码集合独立地根据其各自的语义层次做全图分割，从而得到三个分割图。

2 继续为每个分割区域提取CLIP特征。如此，从三维语义场景渲染的每个像素都具有与其精确语义上下文相匹配的CLIP特征。这种匹配减少了模糊性，提高了基于语义的查询的准确性。

3 在一组图像上获得语义嵌入之后，利用3D高斯来重建3D语义场。


##### 2.4 机器人动作生成

【相关实现】：OpenVLA：https://www.yuque.com/g/rachelwang-ymztf/gk3y0a/sbwavz69gkuugt2y/collaborator/join?token=zuPev49CymJmBv7H&source=doc_collaborator# 
项目地址：https://openvla.github.io/

