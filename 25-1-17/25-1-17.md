#### 记忆Memory机制前沿研究











#### Google 最新 Titans 模型中的长时记忆机制

------

论文标题：Titans: Learning to Memorize at Test Time

论文地址：https://arxiv.org/pdf/2501.00663v1



#### abstract

关于如何有效利用 RNN 和 Attention，人们在近十年进行了广泛的研究工作。虽然循环神经网络旨在将数据压缩到固定大小的内存（称为隐藏状态）中，但 Attention 允许关注整个上下文窗口，捕获所有标记的直接依赖关系。种更准确的依赖关系建模带来了二次成本，将模型限制在固定长度的上下文中。

本文提出了一种新的神经长期记忆模块，并证明该模块具有快速可并行训练的优势。本文将传统 Attention 作为短期记忆，将提出的神经长期记忆模块充当长期记忆。并对比了Titans模型中长期记忆模块不同插入方式的三种变体。



#### Introduction

大多数现有架构将记忆视为由输入引起的神经网络更新，并将学习定义为在给定目标情况下获得有效和有用记忆的过程。这个观点可以帮助我们更好地理解现有范式中的关键差异：例如线性Transformers与Transformer之间的主要区别在于记忆结构以及记忆更新步骤，其中线性Transformers将历史数据压缩到固定大小的矩阵值内存中，而Transformers保留所有历史数据（在上下文长度内）而不进行任何压缩。

虽然线性Transformer和线性RNN（包括状态空间模型State Space Model）都在内存更新步骤中压缩信息，但关键区别在于记忆的结构，其中线性RNN（相对于线性Transformer）使用向量值记忆（相对于矩阵值记忆）。因此，这个观点促使我们问：

- **Q1** 什么构成了记忆的良好结构？
- **Q2** 什么是适当的记忆更新机制？
- **Q3** 什么是好的记忆检索过程？

记忆不是一个单一的过程也不是单一的功能，事实上，记忆是系统的集合——短期记忆、工作记忆、长期记忆——每个系统都有不同的功能且能独立与合作。这一事实促使我们继续提出以下问题：

- **Q4** 如何设计一个包含不同互联记忆模块的高效架构？

最后，存储记忆是一个神经网络过程，需要编码和存储过去的摘要。假设单个向量或矩阵（其参数以线性方式编码数据）足以存储长期历史记录，这可能过于简单化。

- **Q5** 是否需要深度记忆模块来有效地存储/记住很久以前的内容？



#### Preliminaries

文中首先回顾了Transformer中自注意力机制：
$$
Q = x W_Q, \quad K = x W_K, \quad V = x W_V
$$

$$
y_i = \frac{\sum_{j=1}^i \exp\left(Q_i^\top K_j / \sqrt{d_{in}}\right) V_j}{\sum_{\ell=1}^i \exp\left(Q_i^\top K_\ell / \sqrt{d_{in}}\right)}
$$

> **定义变量**：
>
> - 输入变量 x：是输入序列，每个时间步是一个向量，表示模型接收到的特征。
>
> - 投影矩阵 $W_Q, W_k, W_V$ :
>
>   $W_Q$：用于将输入 x 投影为查询向量（Query）
>
>   $W_K$：用于将输入 x 投影为键向量（Key）
>
>   $W_V$：用于将输入 x 投影为值向量（Value）
>
> - 生成的中间向量 :
>
>   Q：查询矩阵，通过 $Q=xW_Q$ 计算得到
>
>   K：键矩阵，通过 $K=xW_K$ 计算得到
>
>   V：值矩阵，通过 $V=xW_V$ 计算得到
>
> 自注意力的核心是计算每个序列元素与其他元素的相似性，然后用这个相似性分布对值向量加权求和，结果用作最终的输出表示。
>
> **具体公式解释**：
>
> 对于第$i$个时间步，输出$y_i$的计算方式如下：
> $$
> y_i = \frac{\sum_{j=1}^i \exp\left(Q_i^\top K_j / \sqrt{d_{in}}\right)}{\sum_{\ell=1}^i \exp\left(Q_i^\top K_\ell / \sqrt{d_{in}}\right)} V_j
> $$
>
> - 查询 $Q$ 与键 $K$ 的点积：
>
>   $Q_i^\top K_j$ : 表示查询向量 $Q_i$ 和键向量 $K_j$ 的点积
>
>   通过点积计算 $i$-位置与 $j$-位置的相关性
>
> - 缩放因子：
>
>   $d_{in}$: 为了缓解点积值过大引起的梯度不稳定问题，进行缩放。这里 $d_{in}$ 是查询和键的维度。
>
> - 指数和归一化：
>
>   $\exp\left(Q_i^\top K_j / \sqrt{d_{in}}\right)$: 将点积相似性值转化为非负数，表示 $i$ 和 $j$ 位置的注意力分数
>
>   分母是所有前 $i$ 个位置的注意力分数的总和，用来归一化
>
> - 值向量加权求和：
>
>   归一化的注意力分数 $\frac{exp⁡(...)}{∑...}$ 对对应的值向量 $V_j$ 进行加权求和，得到输出 $y_i$。
>
> **上下标说明**：
>
> - $Q_i$ : 表示第 $i$ 个位置的查询向量
>
> - $K_j$ : 表示第 $j$ 个位置的键向量
>
> - $\ell$ : 表示归一化时对前 $i$ 个位置的键向量遍历
> - $y_i$ : 表示第 $i$ 个位置的输出
>
> - 求和范围 $\sum_{j=1}^i$ :
>
>   表示只考虑当前及之前的位置，适用于因果注意力机制，确保未来的信息不被使用（例如在语言模型中处理序列预测任务时）
>
> **直观理解**:
>
> - 查询-键相似度：
>
>   通过查询向量 $Q_i$ 和键向量 $K_j$ 计算序列中位置 $i$ 和 $j$ 的相关性。
>
> - 加权求和：
>
>   用这个相似性分布对值向量 $V_j$ 进行加权，得到第 $i$ 个位置的输出。 这种机制允许模型根据上下文动态调整注意力分布，专注于输入序列的相关部分，是 Transformer 的核心计算步骤。

其中 $W_Q,W_K,W_V ∈ R^{𝑑_{in} × 𝑑_{in}}$ 是可学习参数。 尽管其在召回方面具有强大的能力和有效性，但Transformer至少需要 $N × d$ 次操作来计算输出，这导致对于更长的序列，其内存消耗更大，吞吐量更低。

为了降低长序列中softmax注意力机制的内存消耗并提高吞吐量，众多设计出更多通过稀疏化注意力矩阵、近似softmax或开发基于核函数（线性）的注意力机制。在本部分我们重点关注后者——线性注意力机制 Linear Attention，在这种机制中，标准注意力中的softmax被替代为另一种核函数。相应地注意力可以表示为：
$$
y_i 
=
\sum_{j=1}^i
\frac{
\phi(Q_i^\top K_j)
}{
\sum_{\ell=1}^i \phi(Q_i^\top K_\ell)
} V_j
=
\sum_{j=1}^i
\frac{
\phi(Q_i)^\top \phi(K_j)
}{
\sum_{\ell=1}^i \phi(Q_i)^\top \phi(K_\ell)
} V_j
=
\frac{
\phi(Q_i)^\top \sum_{j=1}^i \phi(K_j) V_j
}{
\phi(Q_i)^\top \sum_{\ell=1}^i \phi(K_\ell)
}
$$
> **主要符号解释**：
>
> - 输入向量和投影：
>
>   $$Q_i,K_j,V_j$$ : 查询/键/值 向量（Query/Key/Value），分别表示第 $i$ / $j$ / $j$ 个序列的特征信息
>
>   $\phi(·)$ : 特征映射函数，用于将输入向量投影到一个新的空间，通常是非线性变换
>
> - 点积注意力的改进:
>
>   传统自注意力使用 $exp⁡(Q_i^⊤ K_j / \sqrt{d})$ 来计算点积相似度，而这里引入了特征映射 $ϕ(⋅)$，从而以更加高效的方式计算点积相似度。
>
> - 归一化因子:
>
>   分母 $\sum_{\ell=1}^i \phi(Q_i^\top K_\ell)$ 表示当前 $i$ 个位置的所有键向量 $K_ℓ$ 与查询 $Q_i$ 的相似度总和，用于归一化注意力权重。
>
> - 值加权求和:
>
>   分子 $\sum_{j=1}^i \phi(Q_i)^\top \phi(K_j) V_j$ 表示每个键向量的权重（与 $Q_i$ 的相似度）对对应值向量 $V_j$ 的加权求和。
>
> **逐步分解解释**：
> $$
> y_i 
> =
> \frac{
> \phi(Q_i)^\top \sum_{j=1}^i \phi(K_j) V_j
> }{
> \phi(Q_i)^\top \sum_{\ell=1}^i \phi(K_\ell)
> }
> $$
>
> - 相似度计算：
>
>   $\phi(Q_i)^\top \phi(K_j)$ 表示查询 $Q_i$ 和键 $K_j$ 在特征空间的相似度
>
> - 加权求和：
>
>   将相似度作为权重，对值向量 $V_j$ 进行加权求和，表示通过查询 $Q_i$ 对历史序列的信息聚合。
>
> - 归一化：
>
>   用 $\phi(Q_i)^\top \sum_{\ell=1}^i \phi(K_\ell)$ 对加权求和结果进行归一化，确保输出 $y_i$ 是一个有效的概率分布表示。
>
> - 最终形式：
>
>   通过将分子和分母表示为矩阵形式，进一步优化计算复杂度，适合处理长序列
>
> **优化特点**：
>
> - 特征映射 $\phi(·)$ : 
>
>   使用特征映射 $\phi$ 替代直接点积操作，能够减少计算开销，尤其适用于长序列。常见的 $\phi$ 选择包括核函数或基于稀疏化的近似方法
>
> - 因果结构 : 
>
>   求和范围 $j=1$ 到 $i$，限制了当前输出 $y_i$ 只依赖过去及当前的信息，保证了因果性
>
> - 矩阵形式简化 ：
>
>   通过将加权求和和归一化写为矩阵运算，可以进一步提高实现效率

由于在每一步中都重复使用了 $\sum_{j=1}^i \phi(K_j)$ 和 $\sum_{\ell=1}^i \phi(K_\ell)$ 这两个项，从而实现了更高的吞吐量。当选择核函数 $\phi$ 为单位矩阵时，上述公式也可以写成递归格式，这使得对线性注意力进行高效推理成为可能：
$$
M_t = M_{t-1} + K_t^\top V_t
$$

$$
y_t = Q_t M_t
$$



> $$
> y_i =\frac{\phi(Q_i)^\top \sum_{j=1}^i K_j V_j}{\phi(Q_i)^\top \sum_{j=1}^i K_j}
> $$
>
> 为了避免重复计算累积和，定义累积变量，并将其引入递归形式。
>
> **定义累积矩阵**：
>
> 定义一个累积矩阵 $M_t$，表示到当前时间 $t$ 为止的所有加权和值的累积：
> $$
> M_t = \sum_{j=1}^t K_j^\top V_j
> $$
> 根据递归的性质，我们有：
> $$
> M_t = M_{t-1} + K_j^\top V_j
> $$
> 其中 $M_{t-1}$ 是上一时间步的累积结果，$K_j^\top V_j$ 是当前时间步键和值的贡献。这个递归公式避免了每次都计算和重新求和 $\sum_{j=1}^t$ ，而是通过增量更新。
>
> 根据累积结果 $M_t$ , 可以直接计算出：
> $$
> y_t = Q_t M_t
> $$
> 这是因为归一化因子在特定场景（如选择线性核函数）可以省略，或者被吸收到权重更新中。

现代线性模型将循环神经网络（RNN）的隐藏状态视为一个记忆单元，模型旨在将信息压缩到该记忆单元中。因此，在一般形式的循环神经网络中，隐藏状态可以被视为一个记忆单元，循环（递归）过程可以拆分为读和写操作。即，我们让 $x \in \mathbb{R}^{N \times d_{in}}$ 是输入，$M \in \mathbb{R}^d$ 是记忆单元，$y \in \mathbb{R}^{d_{in}}$ 是输出，那么循环神经网络的一般形式可以定义为：
$$
M_t = f(M_{t-1}, x_t) \hspace{2cm} 
\text{\textcolor{gray}{Write}} \hspace{0.1cm} \text{\textcolor{gray}{Operation}}
$$

$$
y_t = g(M_t, x_t)  \hspace{2.8cm} 
\text{\textcolor{gray}{Read}} \hspace{0.1cm} \text{\textcolor{gray}{Operation}}
$$

其中 $f(...)$ 是读取函数，$g(...)$ 是写入函数。这里的 $M_t$ 表示时间 $t$ 时记忆的状态



















#### Meta 对记忆层的最新研究

------

论文标题：Memory Layers at Scale

论文地址：https://arxiv.org/pdf/2412.09764

##### abstract













