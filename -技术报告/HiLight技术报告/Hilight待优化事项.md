#### 1 模型架构

##### 1.1 CLIP-ViP’ 根据视频长度动态取帧

- 背景：CLIP-ViP‘一直以12帧的固定长度对任何视频进行特征提取

- 问题：当前实现对于长视频固定取帧，相比短视频容易丢失大量信息

- 目标：训练在视频长度上动态取帧的Video Encoder以接纳超长视频数据

##### 1.2 CLIP-ViP‘ 与 Long-CliP 结合

- 背景：CLIP-ViP'具备视频特征提取的连续性，但在长文本与长视频中有效Token与有效Patch训练不充分;Long-CLIP只适用于单帧图像，但其在详细文本描述与充分保留图像细节上有更好的解决方案
- 问题：当前采用双塔结构将两组Encoder并行处理视频特征再Token Mining的方式不如一个集成了两者优点单塔结构效果好

- 目标：参考Long-CLIP相较于CLIP的改进方式，优化CLIP-VIP'模型

##### 1.3 LLM 优化

- 背景：当前核心LLM使用的是在Mini-Gemini中指令微调后的Gemma-2B模型
- 问题：HiLight模型的上限和逻辑能力取决于Gemma-2B模型
- 目标：可以尝试替换使用更强大的语言模型解放模型推理能力上限，例如LLaMa3等

##### 1.4 增加时间与空间的检索功能

- 背景：当前视觉Encoder和LLM的组合仅支持以视频为最小单位的信息理解和对话能力
- 问题：不具备对视频对应时刻对应位置的准确显式地理解，因为训练过程中并没有设计物体对应坐标的回归

- 目标：实现模型可以准确定位时间与空间位置的检索功能





#### 2 模型训练/推理

##### 2.1 长度分组采样器(LengthGroupedSampler) 增加对视频特征的考量

- 背景：长度分组采样器在有效数据中以一种保持一定随机性的方式，将数据集中长度大致相同的特征样本组合在一起进行采样
- 问题：当前长度分组采样器对长度的计算仅考虑文本Token长度，不考虑视频长度
- 目标：实现将视频实际取帧的特征长度纳入长度分组考量范围

##### 2.2 多轮对话训练调优1

- 背景：当前训练数据是以单轮对话设计的
- 问题：对于多个文本单个视频、单个文本多个视频、多轮对话中多个视频的场景，模型未经过充分微调，推理代码也尚未支持这些功能
- 目标：多轮对话场景的数据、训练、推理调优

##### 2.3 多轮对话训练调优2

- 背景：当前推理实现支持单次视频输入，多轮文本对话
- 问题：允许单个或多个视频输入，但是模型只在首次输入接收视频内容，后续文本对话与语言模型无异。模型拥有基于文本的上下文推理，但是没有基于视频的上下文推理。模型不支持同一个上下文关系依赖建模中接收多次视频输入
- 目标：实现真正的视频模态的多轮对话，允许模型分批多次输入视频，允许模型建模视频与视频之间的依赖关系。

##### 2.4 多GPU推理加速

- 背景：一般推理过程以单GPU进行
- 问题：单GPU不足以实现较快地视频问答
- 目标：实现多GPU推理加速





24.8.21更新:

- 多视频多轮对话的充分训练
- 基于视频的Grounding级别的理解，理解视频中特定帧特定区域的信息，真正实现视频的有效检索能力

- 增加模型多模态输入的上下文长度，设计动态采样以不损失精度的清空下低成本推理长视频
- 支持视频流的实时交互的pipeline（当前模型是以对话的形式，必须完整输入视频随后再做出回答。构建实时交互的pipeline可以支持模型在接收视频流对视频状态实时理解的同时，在关键时刻主动与用户发起交互，自主反应。）
