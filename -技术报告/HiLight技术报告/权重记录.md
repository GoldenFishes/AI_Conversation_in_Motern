#### LLM权重

Gemma-2B：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-0.0004, -0.0166,  0.0009,  ...,  0.0073, -0.0013, -0.0022],
        [-0.0259, -0.0041,  0.0066,  ...,  0.0209,  0.0039,  0.0142],
        [ 0.0073,  0.0010, -0.0104,  ...,  0.0034, -0.0061,  0.0017],
        ...,
        [ 0.0080, -0.0072,  0.0030,  ..., -0.0038, -0.0085,  0.0008],
        [-0.0029, -0.0018, -0.0033,  ..., -0.0004, -0.0027, -0.0023],
        [ 0.0060, -0.0029,  0.0023,  ...,  0.0032,  0.0031,  0.0054]])
```

HiLight-2B-LoRA-Merge-stage2-VI100K-1.5epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[ 0.0001, -0.0168,  0.0009,  ...,  0.0071, -0.0011, -0.0022],
        [-0.0255, -0.0045,  0.0077,  ...,  0.0206,  0.0050,  0.0150],
        [ 0.0071,  0.0008, -0.0106,  ...,  0.0034, -0.0062,  0.0014],
        ...,
        [ 0.0080, -0.0073,  0.0034,  ..., -0.0038, -0.0082,  0.0009],
        [-0.0036, -0.0018, -0.0032,  ..., -0.0004, -0.0025, -0.0021],
        [ 0.0059, -0.0031,  0.0025,  ...,  0.0031,  0.0033,  0.0054]])
```

HiLight-2B-LoRA-Merge-stage2-VI100K-3epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-0.0011, -0.0163,  0.0012,  ...,  0.0075, -0.0018, -0.0026],
        [-0.0241, -0.0071,  0.0042,  ...,  0.0210,  0.0065,  0.0165],
        [ 0.0076,  0.0005, -0.0107,  ...,  0.0037, -0.0054,  0.0021],
        ...,
        [ 0.0073, -0.0065,  0.0034,  ..., -0.0037, -0.0089,  0.0006],
        [-0.0009, -0.0061, -0.0066,  ..., -0.0002,  0.0010,  0.0010],
        [ 0.0070, -0.0049,  0.0007,  ...,  0.0033,  0.0047,  0.0068]])
```

HiLight-2B-LoRA-Merge-stage2-VT1K-1epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-0.0012, -0.0163,  0.0012,  ...,  0.0075, -0.0018, -0.0026],
        [-0.0240, -0.0070,  0.0042,  ...,  0.0210,  0.0065,  0.0165],
        [ 0.0076,  0.0005, -0.0107,  ...,  0.0038, -0.0054,  0.0021],
        ...,
        [ 0.0073, -0.0066,  0.0034,  ..., -0.0038, -0.0089,  0.0006],
        [-0.0009, -0.0061, -0.0066,  ..., -0.0002,  0.0010,  0.0010],
        [ 0.0071, -0.0049,  0.0007,  ...,  0.0033,  0.0047,  0.0068]])
```

HiLight-2B-LoRA-Merge-stage2-VT1K-2epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-0.0012, -0.0163,  0.0012,  ...,  0.0075, -0.0018, -0.0026],
        [-0.0240, -0.0070,  0.0042,  ...,  0.0210,  0.0065,  0.0165],
        [ 0.0076,  0.0005, -0.0107,  ...,  0.0037, -0.0054,  0.0021],
        ...,
        [ 0.0073, -0.0066,  0.0034,  ..., -0.0038, -0.0089,  0.0006],
        [-0.0009, -0.0061, -0.0066,  ..., -0.0002,  0.0010,  0.0010],
        [ 0.0070, -0.0050,  0.0007,  ...,  0.0033,  0.0047,  0.0068]])
```

HiLight-2B-LoRA-Merge-stage2-M101K-1epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-2.2459e-03, -1.4232e-02,  3.5268e-03,  ...,  8.5451e-03,
         -1.6542e-03, -3.5375e-03],
        [-2.5060e-02, -5.5162e-03,  5.1087e-03,  ...,  2.0168e-02,
          4.4921e-03,  1.5614e-02],
        [ 1.1922e-02, -5.1256e-03, -1.7130e-02,  ...,  8.0046e-04,
         -5.2441e-03,  4.6027e-03],
        ...,
        [ 3.2909e-03, -4.6869e-05,  1.0994e-02,  ..., -4.7623e-04,
         -8.8218e-03, -2.4865e-03],
        [ 3.4406e-03, -1.0171e-02, -1.2811e-02,  ..., -4.4234e-03,
         -1.4121e-03,  2.1260e-03],
        [ 1.2685e-02, -1.1417e-02, -7.4221e-03,  ..., -6.8873e-04,
          4.4763e-03,  9.7864e-03]])
```

HiLight-2B-LoRA-Merge-stage2-M101K-2epoch：

```python
layers.17.self_attn.q_proj.weight:
tensor([[-0.0030, -0.0133,  0.0047,  ...,  0.0093, -0.0018, -0.0039],
        [-0.0238, -0.0073,  0.0032,  ...,  0.0191,  0.0048,  0.0167],
        [ 0.0137, -0.0069, -0.0193,  ..., -0.0007, -0.0047,  0.0056],
        ...,
        [ 0.0060, -0.0032,  0.0076,  ..., -0.0016, -0.0083, -0.0007],
        [ 0.0073, -0.0141, -0.0177,  ..., -0.0072, -0.0003,  0.0042],
        [ 0.0157, -0.0148, -0.0116,  ..., -0.0031,  0.0050,  0.0114]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VI100K-1.5epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[-3.3055e-03, -1.4068e-02,  5.5168e-03,  ...,  1.0838e-02,
         -2.5442e-04, -5.5974e-03],
        [-2.2141e-02, -6.7853e-03,  7.8881e-04,  ...,  1.6543e-02,
          2.8565e-03,  1.8668e-02],
        [ 8.1857e-03,  6.5695e-05, -1.2311e-02,  ...,  1.8757e-03,
         -6.6180e-03,  2.8242e-03],
        ...,
        [ 5.8373e-03, -4.2170e-03,  8.2150e-03,  ..., -5.1053e-05,
         -6.8468e-03, -2.3054e-03],
        [ 2.8190e-03, -7.0355e-03, -1.3040e-02,  ..., -7.7255e-03,
         -4.4139e-03,  4.6452e-03],
        [ 1.1498e-02, -7.3438e-03, -6.6000e-03,  ..., -3.8946e-03,
          1.2267e-03,  1.2158e-02]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VI100K-3epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[-4.1614e-03, -1.3397e-02,  7.0415e-03,  ...,  1.2264e-02,
          5.9109e-06, -6.5771e-03],
        [-2.1665e-02, -6.9705e-03,  3.5592e-05,  ...,  1.5803e-02,
          2.7007e-03,  1.9003e-02],
        [ 9.3204e-03, -9.8017e-04, -1.4337e-02,  ...,  1.3425e-04,
         -7.0040e-03,  4.0445e-03],
        ...,
        [ 5.2914e-03, -3.7795e-03,  9.1477e-03,  ...,  9.2557e-04,
         -6.5180e-03, -2.7969e-03],
        [ 4.1289e-03, -7.8582e-03, -1.5072e-02,  ..., -9.6715e-03,
         -4.7785e-03,  5.7215e-03],
        [ 1.1358e-02, -6.8390e-03, -6.1948e-03,  ..., -3.8244e-03,
          1.2928e-03,  1.1650e-02]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-1epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[-4.1974e-03, -1.3432e-02,  7.0706e-03,  ...,  1.2228e-02,
          4.7106e-05, -6.6265e-03],
        [-2.1657e-02, -6.9572e-03,  2.2328e-05,  ...,  1.5825e-02,
          2.6802e-03,  1.9017e-02],
        [ 9.3119e-03, -9.7633e-04, -1.4322e-02,  ...,  1.1458e-04,
         -6.9889e-03,  4.0320e-03],
        ...,
        [ 5.2937e-03, -3.7711e-03,  9.1593e-03,  ...,  9.2309e-04,
         -6.5317e-03, -2.7951e-03],
        [ 4.1150e-03, -7.8439e-03, -1.5110e-02,  ..., -9.6563e-03,
         -4.7998e-03,  5.7323e-03],
        [ 1.1400e-02, -6.8032e-03, -6.2165e-03,  ..., -3.7825e-03,
          1.2550e-03,  1.1685e-02]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-2epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[-4.2212e-03, -1.3453e-02,  7.0938e-03,  ...,  1.2211e-02,
          6.5466e-05, -6.6425e-03],
        [-2.1639e-02, -6.9436e-03,  1.0041e-05,  ...,  1.5826e-02,
          2.6698e-03,  1.9042e-02],
        [ 9.3011e-03, -9.8919e-04, -1.4318e-02,  ...,  1.1388e-04,
         -6.9804e-03,  4.0180e-03],
        ...,
        [ 5.2856e-03, -3.7687e-03,  9.1607e-03,  ...,  9.1375e-04,
         -6.5326e-03, -2.7994e-03],
        [ 4.0875e-03, -7.8402e-03, -1.5113e-02,  ..., -9.6504e-03,
         -4.8029e-03,  5.7329e-03],
        [ 1.1397e-02, -6.8099e-03, -6.2201e-03,  ..., -3.7781e-03,
          1.2569e-03,  1.1677e-02]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-3epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[-4.2265e-03, -1.3453e-02,  7.0975e-03,  ...,  1.2210e-02,
          7.1388e-05, -6.6451e-03],
        [-2.1632e-02, -6.9393e-03,  7.1164e-06,  ...,  1.5827e-02,
          2.6659e-03,  1.9049e-02],
        [ 9.2967e-03, -9.9215e-04, -1.4315e-02,  ...,  1.1298e-04,
         -6.9768e-03,  4.0145e-03],
        ...,
        [ 5.2832e-03, -3.7704e-03,  9.1619e-03,  ...,  9.1097e-04,
         -6.5320e-03, -2.7996e-03],
        [ 4.0792e-03, -7.8395e-03, -1.5114e-02,  ..., -9.6490e-03,
         -4.8024e-03,  5.7322e-03],
        [ 1.1399e-02, -6.8085e-03, -6.2177e-03,  ..., -3.7779e-03,
          1.2573e-03,  1.1678e-02]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)M101K-1.5epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[ 0.0067, -0.0249, -0.0093,  ..., -0.0039,  0.0041,  0.0070],
        [-0.0220, -0.0086,  0.0014,  ...,  0.0149,  0.0066,  0.0194],
        [ 0.0113, -0.0041, -0.0171,  ..., -0.0033, -0.0029,  0.0074],
        ...,
        [ 0.0040, -0.0020,  0.0104,  ...,  0.0036, -0.0119, -0.0055],
        [ 0.0012, -0.0061, -0.0080,  ..., -0.0058, -0.0001,  0.0023],
        [ 0.0058, -0.0018,  0.0049,  ...,  0.0053,  0.0022,  0.0037]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)M101K-3epoch

```python
layers.17.self_attn.q_proj.weight:
tensor([[ 7.4288e-03, -2.5930e-02, -1.0057e-02,  ..., -4.9365e-03,
          4.6856e-03,  7.2152e-03],
        [-2.1997e-02, -8.7720e-03,  1.2525e-03,  ...,  1.4768e-02,
          6.6534e-03,  1.9514e-02],
        [ 1.1601e-02, -4.6103e-03, -1.7518e-02,  ..., -3.8104e-03,
         -2.7413e-03,  7.6277e-03],
        ...,
        [ 3.7976e-03, -1.7215e-03,  1.0076e-02,  ...,  3.5188e-03,
         -1.1976e-02, -5.2203e-03],
        [ 1.2591e-03, -6.3570e-03, -8.0517e-03,  ..., -5.8439e-03,
          1.0967e-05,  1.6447e-03],
        [ 6.0645e-03, -2.0152e-03,  4.8419e-03,  ...,  5.2631e-03,
          2.4921e-03,  3.4803e-03]])
```



#### TokenMining权重

所有冻结TokenMining微调的权重都和 ”原始权重valley_20epoch.bin“ 一致为：

```python
tokenizer_projector.1.weight:
tensor([[-0.0391, -0.0338, -0.0191,  ...,  0.0071,  0.0374, -0.0490],
        [-0.0577, -0.0267,  0.0591,  ..., -0.0340,  0.0087,  0.0483],
        [ 0.0088,  0.0399, -0.0532,  ...,  0.1385,  0.0291, -0.0339],
        ...,
        [ 0.0094,  0.0953,  0.0626,  ...,  0.0062,  0.0267, -0.0704],
        [-0.0074,  0.1225, -0.0024,  ...,  0.0163,  0.0447,  0.0170],
        [ 0.0377, -0.0025,  0.0402,  ...,  0.0164,  0.0051, -0.0632]],
```

放开TokenMining微调：
HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VI100K-1.5epoch

```python
tokenizer_projector.1.weight:
tensor([[-0.0405, -0.0382, -0.0216,  ...,  0.0056,  0.0352, -0.0554],
        [-0.0572, -0.0266,  0.0590,  ..., -0.0330,  0.0097,  0.0507],
        [ 0.0098,  0.0431, -0.0510,  ...,  0.1400,  0.0297, -0.0290],
        ...,
        [ 0.0092,  0.0952,  0.0625,  ...,  0.0060,  0.0262, -0.0712],
        [-0.0075,  0.1218, -0.0023,  ...,  0.0160,  0.0441,  0.0160],
        [ 0.0391,  0.0004,  0.0420,  ...,  0.0178,  0.0062, -0.0587]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VI100K-3epoch

```python
tokenizer_projector.1.weight:
tensor([[-0.0405, -0.0380, -0.0216,  ...,  0.0057,  0.0350, -0.0562],
        [-0.0570, -0.0265,  0.0591,  ..., -0.0332,  0.0096,  0.0505],
        [ 0.0100,  0.0435, -0.0505,  ...,  0.1402,  0.0300, -0.0273],
        ...,
        [ 0.0092,  0.0953,  0.0626,  ...,  0.0060,  0.0263, -0.0710],
        [-0.0076,  0.1218, -0.0022,  ...,  0.0159,  0.0442,  0.0161],
        [ 0.0393,  0.0003,  0.0420,  ...,  0.0178,  0.0063, -0.0581]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-1epoch【？为什么tokenmining没有变化】

```python
tokenizer_projector.1.weight:
tensor([[-0.0404, -0.0380, -0.0216,  ...,  0.0057,  0.0351, -0.0562],
        [-0.0570, -0.0265,  0.0591,  ..., -0.0332,  0.0096,  0.0505],
        [ 0.0100,  0.0435, -0.0505,  ...,  0.1402,  0.0300, -0.0273],
        ...,
        [ 0.0091,  0.0953,  0.0627,  ...,  0.0060,  0.0262, -0.0710],
        [-0.0076,  0.1219, -0.0022,  ...,  0.0160,  0.0441,  0.0161],
        [ 0.0393,  0.0003,  0.0420,  ...,  0.0178,  0.0063, -0.0581]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-2epoch【？为什么tokenmining没有变化】

```python
tokenizer_projector.1.weight:
tensor([[-0.0404, -0.0380, -0.0216,  ...,  0.0057,  0.0351, -0.0562],
        [-0.0570, -0.0265,  0.0591,  ..., -0.0332,  0.0096,  0.0505],
        [ 0.0100,  0.0435, -0.0505,  ...,  0.1402,  0.0300, -0.0272],
        ...,
        [ 0.0091,  0.0954,  0.0627,  ...,  0.0060,  0.0263, -0.0711],
        [-0.0077,  0.1219, -0.0022,  ...,  0.0160,  0.0441,  0.0161],
        [ 0.0393,  0.0003,  0.0420,  ...,  0.0178,  0.0063, -0.0581]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)VT1K-3epoch

```python
tokenizer_projector.1.weight:
tensor([[-0.0404, -0.0380, -0.0216,  ...,  0.0057,  0.0350, -0.0562],
        [-0.0570, -0.0265,  0.0591,  ..., -0.0332,  0.0096,  0.0505],
        [ 0.0100,  0.0435, -0.0505,  ...,  0.1402,  0.0300, -0.0272],
        ...,
        [ 0.0091,  0.0954,  0.0627,  ...,  0.0060,  0.0263, -0.0711],
        [-0.0076,  0.1219, -0.0022,  ...,  0.0160,  0.0441,  0.0161],
        [ 0.0393,  0.0003,  0.0420,  ...,  0.0178,  0.0063, -0.0581]])
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)M101K-1.5epoch

```python
tokenizer_projector.1.weight:
tensor([[-0.0404, -0.0356, -0.0199,  ...,  0.0059,  0.0380, -0.0518],
        [-0.0585, -0.0255,  0.0584,  ..., -0.0344,  0.0095,  0.0484],
        [ 0.0097,  0.0400, -0.0524,  ...,  0.1389,  0.0290, -0.0333],
        ...,
        [ 0.0109,  0.0944,  0.0634,  ...,  0.0085,  0.0236, -0.0680],
        [-0.0081,  0.1218, -0.0025,  ...,  0.0159,  0.0446,  0.0164],
        [ 0.0364, -0.0018,  0.0397,  ...,  0.0153,  0.0069, -0.0651]],
```

HiLight-2B-LoRA-Merge-stage2-tune(TM+LLM)M101K-3epoch

```python
tokenizer_projector.1.weight:
tensor([[-0.0403, -0.0351, -0.0201,  ...,  0.0060,  0.0381, -0.0517],
        [-0.0582, -0.0256,  0.0585,  ..., -0.0343,  0.0091,  0.0485],
        [ 0.0103,  0.0402, -0.0522,  ...,  0.1391,  0.0291, -0.0329],
        ...,
        [ 0.0108,  0.0939,  0.0636,  ...,  0.0082,  0.0239, -0.0684],
        [-0.0084,  0.1216, -0.0028,  ...,  0.0156,  0.0449,  0.0160],
        [ 0.0361, -0.0012,  0.0394,  ...,  0.0154,  0.0069, -0.0651]],
```



```
model.token_mining.tokenizer_projector.0.weight
model.token_mining.tokenizer_projector.0.bias
model.token_mining.tokenizer_projector.1.weight
model.token_mining.tokenizer_projector.1.bias
model.token_mining.tokenizer_projector.3.weight
model.token_mining.tokenizer_projector.3.bias
```

```
Updated key: model.token_mining.tokenizer_projector.0.bias
Updated key: model.token_mining.tokenizer_projector.1.weight
Updated key: model.token_mining.tokenizer_projector.1.bias
Updated key: model.token_mining.tokenizer_projector.1.lora_A.default.weight
Updated key: model.token_mining.tokenizer_projector.1.lora_B.default.weight
Updated key: model.token_mining.tokenizer_projector.3.weight
Updated key: model.token_mining.tokenizer_projector.3.bias
Updated key: model.token_mining.tokenizer_projector.3.lora_A.default.weight
Updated key: model.token_mining.tokenizer_projector.3.lora_B.default.weight
```

```
model.layers.17.self_attn.k_proj.weight
model.layers.17.self_attn.v_proj.weight
model.layers.17.self_attn.o_proj.weight
model.layers.17.mlp.gate_proj.weight
model.layers.17.mlp.up_proj.weight
model.layers.17.mlp.down_proj.weight
model.layers.17.input_layernorm.weight
model.layers.17.post_attention_layernorm.weight
```

tensor([[-0.0087, -0.0474, -0.0329,  ..., -0.0257, -0.0118, -0.0437],
        [-0.0279, -0.0418, -0.0441,  ...,  0.0229,  0.0173,  0.0040],
        [-0.0196,  0.0245, -0.0079,  ..., -0.0072, -0.0348,  0.0222],
        ...,
        [ 0.0061,  0.0184, -0.0434,  ..., -0.0238,  0.0099, -0.0334],
        [-0.0444, -0.0083, -0.0341,  ..., -0.0455,  0.0218,  0.0209],
        [ 0.0475,  0.0041,  0.0022,  ...,  0.0117,  0.0217,  0.0340]],
